\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{indentfirst}

\title{HW 3}
\author{Gavin Lesher}
\date{\today}
\begin{document}
    \maketitle
    \clearpage

    \section*{Q1: Prove Bernoulli Na誰ve Bayes has a linear \\decision boundary}
        \begin{flalign}
            \intertext{First, we start with the original inequality:}
            1 &< \frac{\theta_1 \prod_{i=1}^d \theta_{i1}^{x_i} (1-\theta_{i1})^{1-x_i}}
                {\theta_0 \prod_{i=1}^d \theta_{i0}^{x_i} (1-\theta_{i0})^{1-x_i}}
            \\
            \intertext{Take the log of both sides:}
            \quad\log (1) &< \log \left(\frac{\theta_1 \prod_{i=1}^d \theta_{i1}^{x_i} (1-\theta_{i1})^{1-x_i}}
                {\theta_0 \prod_{i=1}^d \theta_{i0}^{x_i} (1-\theta_{i0})^{1-x_i}}\right)
            \\
            \intertext{Group like exponents:}
            0 &< \log\left(\frac{\theta_1}{\theta_0}\right) + 
                \log\left(\prod_{i=1}^d \frac{\theta_{i1}}{\theta_{i1}}^{x_i} \frac{1-\theta_{i1}}{1-\theta_{i0}}^{1-x_i}\right)
            \\
            0 &< \log\left(\frac{\theta_1}{\theta_0}\right) + 
                \sum_{i=1}^d (x_i)\log\left(\frac{\theta_{i1}}{\theta_{i0}}\right) + 
                    (1-x_i)\log\left(\frac{1-\theta_{i1}}{1-\theta_{i0}}\right)
            \\
            \intertext{Bernoulli Na誰ve Bayes in $b + \sum_{i=1}^d x_i w_i > 0$ form:}
            0 &<  \underbrace{\log\left(\frac{\theta_1}{\theta_0}\right) + 
                \sum_{i=1}^d \log\left(\frac{1-\theta_{i1}}{1-\theta_{i0}}\right)}_b + 
                \sum_{i=1}^d x_i 
                    \underbrace{\left(
                    \log\left(\frac{\theta_{i1}}{\theta_{i0}}\right) -
                    \log\left(\frac{1-\theta_{i1}}{1-\theta_{i0}}\right)
                    \right)}_{w_i}
        \end{flalign}


    \section*{Q2: Duplicate Features in Na誰ve Bayes}
        \textbf{First, we will simplify $P(y=1|X_1=x_1) > P(y=0|X_1=x_1)$:}
        \begin{align}
            P(y=1|X_1=x_1) &> P(y=0|X_1=x_1)\\
            \intertext{\qquad Using Baye's rule:}
            \frac{P(X_1=x_1|y=1)P(y=1)}{P(X_1=x_1)} &>\frac{P(X_1=x_1|y=0)P(y=0)}{P(X_1=x_1)}\\
            \intertext{\qquad Since $P(y=1) = P(y=0)$:}
            P(X_1=x_1|y=1) &> P(X_1=x_0|y=0) \label{eq:8}
        \end{align}


        \noindent\textbf{ Next, we prove that $P(y=1|X_1=x_1, X_2=x_2) > P(y=1|X_1=x_1)$:}
        
        \begin{align}
            \intertext{\qquad Baye's Rule:}
            \frac{P(X_1=x_1, X_2=x_2|y=1)P(y=1)}{P(X_1=x_1, X_2=x_2)} 
                &> \frac{P(X_1=x_1|y=1)P(y=1)}{P(X_1=x_1)} \\
            \intertext{\qquad With the na誰ve bayes assumtion of conditional independence:}
            \frac{P(X_1=x_1|y=1)P(X_2=x_2|y=1)}{P(X_1=x_1, X_2=x_2)} 
                &> \frac{P(X_1=x_1|y=1)}{P(X_1=x_1)} \\
            \frac{P(X_2=x_2|y=1)}{P(X_1=x_1|X_2=x_2)}
                &> \frac{1}{P(X_1=x_1)}\\
            \intertext{\qquad Expanding the denominators:}
            \frac{P(X_2=x_2|y=1)}{\sum_y P(X_1=x_1|y)P(X_2=x_2|y)P(y)} 
                &> \frac{1}{\sum_y P(X_1=x_1|y)P(y)}\\
            \intertext{\qquad Because $P(X_2=x_2|y)$ is equivalent to $P(X_1=x_1|y)$, we can treat $X_1$ and $X_2$ terms as the same.}
            \intertext{\qquad \qquad We flip the fractions for easy manipulation first:}
            \frac{\sum_y P(X_1=x_1|y)}{1}
                &> \frac{\sum_y P(X_1=x_1|y)P(X_2=x_2|y)}{P(X_2=x_2|y=1)} \\
            \intertext{\qquad Which expands to:}
            P(X_1=x_1|y=0) &+ P(X_1=x_1|y=1)
                > \nonumber\\ 
                \frac{P(X_1=x_1|y=0)P(X_2=x_2|y=0)}{P(X_2=x_2|y=1)} &+ \frac{P(X_1=x_1|y=1)P(X_2=x_2|y=1)}{P(X_2=x_2|y=1)} \nonumber \\
            &\nonumber\\
            P(X_1=x_1|y=0) &> \frac{P(X_1=x_1|y=0)P(X_2=x_2|y=0)}{P(X_2=x_2|y=1)}\nonumber\\
            &\nonumber\\
            P(X_2=x_2|y=1) &> P(X_2=x_2|y=0)
        \end{align}
        Which is the equivalence we proved in equation \eqref{eq:8}, and proves the original inequality.

    \clearpage
    \section*{Q4: Learning Rate}
        The default step size seems to be the best learning rate of all the ones tested. 
        With the learning rate too small (0.0001), the NN does not make progress on the backpropogation because the changes from the gradient are too small to compound effectively.
        For the larger step sizes (5 \& 10), The plots are much more jumpy, as the NN overcorrects because of the high step size. 
        The plot for a step size of 10 shows that with a high enough step size, you really don't make much progress at all.
        I believe this to be the NN oscilating with the gradient.

        If the max epochs were increased, I would expect the plots for step size 10 and .00001 to not change. 
        The plot 5 might converge a little bit more, but I find that unlikely, 
        and the plot for .01 will converge further for sure.

        \textit{All plots can be found in the Appendix}



    \clearpage
    \section*{Appendix}
        \begin{figure}[h]
            \includegraphics[width=\textwidth]{"Figure_0001.png"}
            \caption{Step Size 0.0001}
        \end{figure}
        \begin{figure}[h!]
            \includegraphics[width=\textwidth]{"Figure_01.png"}
            \caption{Step Size 0.01 (Default)}
        \end{figure}
        \begin{figure}[t]
            \includegraphics[width=\textwidth]{"Figure_5.png"}
            \caption{Step Size 5}
        \end{figure}
        \begin{figure}[b]
            \includegraphics[width=\textwidth]{"Figure_10.png"}
            \caption{Step Size 10}
        \end{figure}
        \clearpage

        



    
            






\end{document}